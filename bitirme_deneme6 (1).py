# -*- coding: utf-8 -*-
"""bitirme_deneme6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vuGLuJ7xHs98voOvUMFNd-_0wZy9hB4a
"""

from google.colab import drive
drive.mount('/content/drive')

# kullanacağım algoritmalar.
# log.reg - svm - knn - decision tree - rand. forest - LightGBM
# rnn - cnn(conv1d kullan tek boyutluya indirge!!) - lstm - gru - mlp deeplearning

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

red_wine_data = pd.read_csv('/content/drive/My Drive/wine_quality_dataset/winequality-red.csv', sep=';')
white_wine_data = pd.read_csv('/content/drive/My Drive/wine_quality_dataset/winequality-white.csv', sep=';')

# Kırmızı ve beyaz şaraplara ayırıcı bir sütun ekleme
red_wine_data['type'] = 'red'
white_wine_data['type'] = 'white'

# Veri setlerini birleştirme
combined_data = pd.concat([red_wine_data, white_wine_data], axis=0).reset_index(drop=True)

# Eksik değer kontrolü
print("Eksik Değer Kontrolü:\n", combined_data.isnull().sum())

#  (type) one-hot encode
combined_data_encoded = pd.get_dummies(combined_data, columns=['type'], drop_first=True)

# Özellik ve hedef değişkenleri ayır
X = combined_data_encoded.drop('quality', axis=1)
y = combined_data_encoded['quality']

# Eğitim ve test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Özellikleri ölçeklendirme
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Veri Hazırlama Tamamlandı!")

from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# Özellik seçimi işlemi
n_features = 8  # Seçilecek en önemli özellik sayısı
rfe_selector = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=n_features)

# Eğitim verisinde özellik seçimi
X_train_selected = rfe_selector.fit_transform(X_train_scaled, y_train)

# Test verisinde aynı özellikleri kullan
X_test_selected = rfe_selector.transform(X_test_scaled)


selected_features = X.columns[rfe_selector.support_]
print("Seçilen Özellikler:", selected_features)

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# classweight hesaplama
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(zip(np.unique(y_train), class_weights))
print("Sınıf Ağırlıkları:", class_weights_dict)

import seaborn as sns
import matplotlib.pyplot as plt
# Sınıf birleştirme
y_train_merged = y_train.replace({3: 4, 9: 8})
y_test_merged = y_test.replace({3: 4, 9: 8})

# Birleşmiş sınıfların dağılımı!!
sns.countplot(x=y_train_merged)
plt.title("Birleştirilmiş Sınıf Dağılımı (Eğitim Seti)")
plt.show()

print("Sınıfların Birleştirilmesinden Sonra Eğitim Verisi Dağılımı:\n", y_train_merged.value_counts())

# Temel Logistic Regression !!
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report


logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train_scaled, y_train)


y_pred_logistic = logistic_model.predict(X_test_scaled)


logistic_accuracy = accuracy_score(y_test, y_pred_logistic)
logistic_classification_report = classification_report(y_test, y_pred_logistic)

print("Logistic Regression Accuracy (Temel Model):", logistic_accuracy)
print("Logistic Regression Classification Report:\n", logistic_classification_report)

# Logistic Regression (class_weight ve feature selection ile) !!

from sklearn.linear_model import LogisticRegression

logistic_model_cw_fs = LogisticRegression(max_iter=1000, class_weight=class_weights_dict)
logistic_model_cw_fs.fit(X_train_selected, y_train)
y_pred_logistic_cw_fs = logistic_model_cw_fs.predict(X_test_selected)


print("Logistic Regression (Class Weight + Feature Selection) Accuracy:", accuracy_score(y_test, y_pred_logistic_cw_fs))
print("Classification Report:\n", classification_report(y_test, y_pred_logistic_cw_fs))

# Logistic Regression modeli (sınıf birleştirme ile)
logistic_model_merged = LogisticRegression(max_iter=1000)
logistic_model_merged.fit(X_train_scaled, y_train_merged)
y_pred_logistic_merged = logistic_model_merged.predict(X_test_scaled)


print("Logistic Regression (Sınıf Birleştirme) Accuracy:", accuracy_score(y_test_merged, y_pred_logistic_merged))
print("Classification Report:\n", classification_report(y_test_merged, y_pred_logistic_merged))

from sklearn.tree import DecisionTreeClassifier

# Decision Tree modeli
decision_tree_model = DecisionTreeClassifier(random_state=42)
decision_tree_model.fit(X_train_scaled, y_train)


y_pred_decision_tree = decision_tree_model.predict(X_test_scaled)


decision_tree_accuracy = accuracy_score(y_test, y_pred_decision_tree)
decision_tree_classification_report = classification_report(y_test, y_pred_decision_tree)

print("Decision Tree Accuracy (Temel Model):", decision_tree_accuracy)
print("Decision Tree Classification Report:\n", decision_tree_classification_report)

# Decision Tree Modeli (class_weight ve seçilen özelliklerle)
decision_tree_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)
decision_tree_model.fit(X_train_selected, y_train)  # Seçilen özelliklerle eğitiyorzu
y_pred_decision_tree = decision_tree_model.predict(X_test_selected)


accuracy_dt = accuracy_score(y_test, y_pred_decision_tree)
report_dt = classification_report(y_test, y_pred_decision_tree)

print("Decision Tree Accuracy:", accuracy_dt)
print("Decision Tree Classification Report:\n", report_dt)

# Decision Tree Class Merged ile !!
decision_tree_merged = DecisionTreeClassifier(random_state=42)
decision_tree_merged.fit(X_train_scaled, y_train_merged)
y_pred_merged = decision_tree_merged.predict(X_test_scaled)


accuracy_merged = accuracy_score(y_test_merged, y_pred_merged)
report_merged = classification_report(y_test_merged, y_pred_merged)

print("Decision Tree Accuracy (Merged Classes):", accuracy_merged)
print("Decision Tree Classification Report (Merged Classes):\n", report_merged)

from sklearn.ensemble import RandomForestClassifier

# Random Forest modeli
random_forest_model = RandomForestClassifier(random_state=42)
random_forest_model.fit(X_train_scaled, y_train)

# Test verileri üzerinde tahmin
y_pred_rf = random_forest_model.predict(X_test_scaled)


rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_classification_report = classification_report(y_test, y_pred_rf)

print("Random Forest Accuracy (Temel Model):", rf_accuracy)
print("Random Forest Classification Report:\n", rf_classification_report)

# Random Forest modeli (class weight ve feature selection ile)
random_forest_cw_fs = RandomForestClassifier(class_weight='balanced', random_state=42)
random_forest_cw_fs.fit(X_train_selected, y_train)
y_pred_random_forest_cw_fs = random_forest_cw_fs.predict(X_test_selected)


print("Random Forest (Class Weight + Feature Selection) Accuracy:", accuracy_score(y_test, y_pred_random_forest_cw_fs))
print("Classification Report:\n", classification_report(y_test, y_pred_random_forest_cw_fs))

# Random Forest modeli (sınıf birleştirme ile)
random_forest_merged = RandomForestClassifier(random_state=42)
random_forest_merged.fit(X_train_scaled, y_train_merged)
y_pred_random_forest_merged = random_forest_merged.predict(X_test_scaled)


print("Random Forest (Sınıf Birleştirme) Accuracy:", accuracy_score(y_test_merged, y_pred_random_forest_merged))
print("Classification Report:\n", classification_report(y_test_merged, y_pred_random_forest_merged))

from sklearn.neighbors import KNeighborsClassifier

# KNN modeli
knn_model = KNeighborsClassifier()
knn_model.fit(X_train_scaled, y_train)

y_pred_knn = knn_model.predict(X_test_scaled)

knn_accuracy = accuracy_score(y_test, y_pred_knn)
knn_classification_report = classification_report(y_test, y_pred_knn)

print("KNN Accuracy (Temel Model):", knn_accuracy)
print("KNN Classification Report:\n", knn_classification_report)

# KNN modeli (class weight uygulanamaz!!!, sadece feature selection)
knn_cw_fs = KNeighborsClassifier(n_neighbors=5)
knn_cw_fs.fit(X_train_selected, y_train)
y_pred_knn_cw_fs = knn_cw_fs.predict(X_test_selected)


print("KNN (Feature Selection) Accuracy:", accuracy_score(y_test, y_pred_knn_cw_fs))
print("Classification Report:\n", classification_report(y_test, y_pred_knn_cw_fs))

# KNN modeli (sınıf birleştirme ile)
knn_merged = KNeighborsClassifier(n_neighbors=5)
knn_merged.fit(X_train_scaled, y_train_merged)
y_pred_knn_merged = knn_merged.predict(X_test_scaled)


print("KNN (Sınıf Birleştirme) Accuracy:", accuracy_score(y_test_merged, y_pred_knn_merged))
print("Classification Report:\n", classification_report(y_test_merged, y_pred_knn_merged))

from sklearn.svm import SVC

# SVM modeli
svm_model = SVC(random_state=42)
svm_model.fit(X_train_scaled, y_train)


y_pred_svm = svm_model.predict(X_test_scaled)


svm_accuracy = accuracy_score(y_test, y_pred_svm)
svm_classification_report = classification_report(y_test, y_pred_svm)

print("SVM Accuracy (Temel Model):", svm_accuracy)
print("SVM Classification Report:\n", svm_classification_report)

# SVM modeli (class weight ve feature selection ile)
svm_cw_fs = SVC(kernel='linear', class_weight='balanced', random_state=42)
svm_cw_fs.fit(X_train_selected, y_train)
y_pred_svm_cw_fs = svm_cw_fs.predict(X_test_selected)


print("SVM (Class Weight + Feature Selection) Accuracy:", accuracy_score(y_test, y_pred_svm_cw_fs))
print("Classification Report:\n", classification_report(y_test, y_pred_svm_cw_fs))

# SVM modeli (sınıf birleştirme ile)
svm_merged = SVC(kernel='linear', random_state=42)
svm_merged.fit(X_train_scaled, y_train_merged)
y_pred_svm_merged = svm_merged.predict(X_test_scaled)


print("SVM (Sınıf Birleştirme) Accuracy:", accuracy_score(y_test_merged, y_pred_svm_merged))
print("Classification Report:\n", classification_report(y_test_merged, y_pred_svm_merged))

from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score

# Temel LightGBM modeli
lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train_scaled, y_train)


y_pred_lgbm = lgbm_model.predict(X_test_scaled)


accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)
report_lgbm = classification_report(y_test, y_pred_lgbm)

print("LightGBM Temel Model Accuracy:", accuracy_lgbm)
print("LightGBM Temel Model Classification Report:\n", report_lgbm)

# LightGBM modeli (class weight ve feature selection ile)
import lightgbm as lgb

lgbm_cw_fs = lgb.LGBMClassifier(class_weight='balanced', random_state=42)
lgbm_cw_fs.fit(X_train_selected, y_train)
y_pred_lgbm_cw_fs = lgbm_cw_fs.predict(X_test_selected)

# Performans değerlendirme
print("LightGBM (Class Weight + Feature Selection) Accuracy:", accuracy_score(y_test, y_pred_lgbm_cw_fs))
print("Classification Report:\n", classification_report(y_test, y_pred_lgbm_cw_fs))

# LightGBM modeli (sınıf birleştirme ile)
lgbm_merged = lgb.LGBMClassifier(random_state=42)
lgbm_merged.fit(X_train_scaled, y_train_merged)
y_pred_lgbm_merged = lgbm_merged.predict(X_test_scaled)


print("LightGBM (Sınıf Birleştirme) Accuracy:", accuracy_score(y_test_merged, y_pred_lgbm_merged))
print("Classification Report:\n", classification_report(y_test_merged, y_pred_lgbm_merged))

from imblearn.over_sampling import SMOTE

# SMOTE uygulama
# Setting k_neighbors to a smaller value, like '3', to ensure it's less than the smallest minority class size.!!!!!!!!!!!!!!!!??
smote = SMOTE(random_state=42, k_neighbors=3)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"SMOTE sonrası eğitim veri seti boyutu: {X_train_smote.shape[0]} örnek")

# Logistic Regression için hiperparametre opt.
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# Sınıf birleştirme
y_train_smote_merged = y_train_smote.replace({3: 4, 9: 8})
y_test_merged = y_test.replace({3: 4, 9: 8})

# Logistic Regression için hiperparametre aralığı
param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10, 100],  # Düzenleme katsayıları
    'solver': ['liblinear', 'lbfgs'],  # Çözümleyiciler
    'penalty': ['l2']  # L1 düzenlemesi liblinear ile uyumlu değilmiş!!!
}

# Logistic Regression için Grid Search
grid_logistic = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42),
                             param_grid=param_grid_logistic, scoring='accuracy', cv=5, n_jobs=-1)


grid_logistic.fit(X_train_smote, y_train_smote_merged)


print("En iyi hiperparametreler (Logistic Regression):", grid_logistic.best_params_)
print("En iyi doğruluk (Logistic Regression):", grid_logistic.best_score_)


y_pred_logistic_optimized = grid_logistic.best_estimator_.predict(X_test_scaled)


accuracy_logistic_optimized = accuracy_score(y_test_merged, y_pred_logistic_optimized)
print("Logistic Regression (Sınıf Birleştirme + Optimizasyon) Accuracy:", accuracy_logistic_optimized)


report_logistic_optimized = classification_report(y_test_merged, y_pred_logistic_optimized, zero_division=1)
print("Logistic Regression (Sınıf Birleştirme + Optimizasyon) Classification Report:\n", report_logistic_optimized)

# Decision Tree için hiperparametre opt.
from sklearn.tree import DecisionTreeClassifier

# Sınıf birleştirme
y_train_smote_merged = y_train_smote.replace({3: 4, 9: 8})
y_test_merged = y_test.replace({3: 4, 9: 8})

# Decision Tree için hiperparametre aralığı
param_grid_tree = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [3, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Grid Search
grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=42),
                         param_grid=param_grid_tree, scoring='accuracy', cv=5, n_jobs=-1)

# Eğitim
grid_tree.fit(X_train_smote, y_train_smote_merged)

# En iyi parametreler ve doğruluk skoru
print("En iyi hiperparametreler (Decision Tree):", grid_tree.best_params_)
print("En iyi doğruluk (Decision Tree):", grid_tree.best_score_)

# Test setinde tahmin
y_pred_tree_optimized = grid_tree.best_estimator_.predict(X_test_scaled)

# Performans değerlendirme
accuracy_tree_optimized = accuracy_score(y_test_merged, y_pred_tree_optimized)
print("Decision Tree (Sınıf Birleştirme + Optimizasyon) Accuracy:", accuracy_tree_optimized)

# Classification Report
report_tree_optimized = classification_report(y_test_merged, y_pred_tree_optimized, zero_division=1)
print("Decision Tree (Sınıf Birleştirme + Optimizasyon) Classification Report:\n", report_tree_optimized)

# Random Forest için hiperparametre opt.
from sklearn.ensemble import RandomForestClassifier

# Random Forest için hiperparametre aralığı
param_grid_rf = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5, 10],  # Daha küçük max_depth
    'min_samples_split': [5],
    'min_samples_leaf': [3],
    'bootstrap': [True]
}

# Grid Search
grid_rf = GridSearchCV(RandomForestClassifier(random_state=42),
                       param_grid=param_grid_rf, scoring='accuracy', cv=3, n_jobs=-1)


grid_rf.fit(X_train_smote, y_train_smote_merged)


print("En iyi hiperparametreler (Random Forest):", grid_rf.best_params_)
print("En iyi doğruluk (Random Forest):", grid_rf.best_score_)

# Test setinde tahmin
y_pred_rf_optimized = grid_rf.best_estimator_.predict(X_test_scaled)


accuracy_rf_optimized = accuracy_score(y_test_merged, y_pred_rf_optimized)
print("Random Forest (Sınıf Birleştirme + Optimizasyon) Accuracy:", accuracy_rf_optimized)


report_rf_optimized = classification_report(y_test_merged, y_pred_rf_optimized, zero_division=1)
print("Random Forest (Sınıf Birleştirme + Optimizasyon) Classification Report:\n", report_rf_optimized)

# KNN için hiperparametre opt.
from sklearn.neighbors import KNeighborsClassifier

# KNN için hiperparametre aralığı
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}


grid_knn = GridSearchCV(KNeighborsClassifier(),
                        param_grid=param_grid_knn, scoring='accuracy', cv=5, n_jobs=-1)


grid_knn.fit(X_train_smote, y_train_smote_merged)


print("En iyi hiperparametreler (KNN):", grid_knn.best_params_)
print("En iyi doğruluk (KNN):", grid_knn.best_score_)

# Test setinde tahmin
y_pred_knn_optimized = grid_knn.best_estimator_.predict(X_test_scaled)


accuracy_knn_optimized = accuracy_score(y_test_merged, y_pred_knn_optimized)
print("KNN (Sınıf Birleştirme + Optimizasyon) Accuracy:", accuracy_knn_optimized)


report_knn_optimized = classification_report(y_test_merged, y_pred_knn_optimized, zero_division=1)
print("KNN (Sınıf Birleştirme + Optimizasyon) Classification Report:\n", report_knn_optimized)

# SVM için hiperparametre opt.
from sklearn.svm import SVC

# SVM için hiperparametre aralığı
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# Grid Search
grid_svm = GridSearchCV(SVC(random_state=42),
                        param_grid=param_grid_svm, scoring='accuracy', cv=5, n_jobs=-1)

# Eğitim
grid_svm.fit(X_train_scaled, y_train.replace({3: 4, 9: 8}))

# En iyi parametreler ve doğruluk skoru
print("En iyi hiperparametreler (SVM):", grid_svm.best_params_)
print("En iyi doğruluk (SVM):", grid_svm.best_score_)

# Test setinde tahmin
y_pred_svm_optimized = grid_svm.best_estimator_.predict(X_test_scaled)

# Performans değerlendirme
accuracy_svm_optimized = accuracy_score(y_test.replace({3: 4, 9: 8}), y_pred_svm_optimized)
print("SVM (Sınıf Birleştirme + Optimizasyon) Accuracy:", accuracy_svm_optimized)

# Classification Report
report_svm_optimized = classification_report(y_test.replace({3: 4, 9: 8}), y_pred_svm_optimized, zero_division=1)
print("SVM (Sınıf Birleştirme + Optimizasyon) Classification Report:\n", report_svm_optimized)

from sklearn.model_selection import GridSearchCV

# LightGBM için hiperparametre aralığı
param_grid_lgbm = {
    'n_estimators': [100],  # Daha az değer
    'learning_rate': [0.05],  # Tek bir öğrenme oranı
    'max_depth': [5, 10],  # Daha az derinlik seçeneği
    'boosting_type': ['gbdt']  # Sadece 'gbdt' tipi kullanıldı
}

# Grid Search
grid_lgbm = GridSearchCV(LGBMClassifier(random_state=42),
                         param_grid=param_grid_lgbm, scoring='accuracy', cv=5, n_jobs=-1)

# Eğitim
grid_lgbm.fit(X_train_scaled, y_train.replace({3: 4, 9: 8}))

# En iyi parametreler ve doğruluk skoru
print("En iyi hiperparametreler (LightGBM):", grid_lgbm.best_params_)
print("En iyi doğruluk (LightGBM):", grid_lgbm.best_score_)

# Test setinde tahmin
y_pred_lgbm_optimized = grid_lgbm.best_estimator_.predict(X_test_scaled)

# Performans değerlendirme
accuracy_lgbm_optimized = accuracy_score(y_test.replace({3: 4, 9: 8}), y_pred_lgbm_optimized)
print("LightGBM (Sınıf Birleştirme + Optimizasyon) Accuracy:", accuracy_lgbm_optimized)

# Classification Report
report_lgbm_optimized = classification_report(y_test.replace({3: 4, 9: 8}), y_pred_lgbm_optimized, zero_division=1)
print("LightGBM (Sınıf Birleştirme + Optimizasyon) Classification Report:\n", report_lgbm_optimized)

from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization, Input
from keras.callbacks import EarlyStopping
from keras.optimizers import RMSprop
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Label Encoding
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Class weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)
class_weight_dict = dict(zip(np.unique(y_train_encoded), class_weights))


activation_functions = ['relu', 'sigmoid', 'tanh']

for activation in activation_functions:
    print(f"\n--- MLP ile {activation.upper()} aktivasyonu ---")

    # MLP Modeli
    mlp_model = Sequential()
    mlp_model.add(Input(shape=(X_train_scaled.shape[1],)))  # Giriş katmanı
    mlp_model.add(Dense(256, activation=activation))
    mlp_model.add(BatchNormalization())  # Normalizasyon
    mlp_model.add(Dropout(0.3))  # Aşırı öğrenmeyi engellemek için dropout
    mlp_model.add(Dense(128, activation=activation))
    mlp_model.add(Dropout(0.3))
    mlp_model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))  # Çıkış katmanı (çok sınıflı sınıflandırma)

    # Model derleme
    mlp_model.compile(optimizer=RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Early Stopping???
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


    mlp_model.fit(X_train_scaled, y_train_encoded, validation_split=0.2, epochs=50, batch_size=32,
                  class_weight=class_weight_dict, callbacks=[early_stopping])


    y_pred_mlp = np.argmax(mlp_model.predict(X_test_scaled), axis=1)
    mlp_accuracy = accuracy_score(y_test_encoded, y_pred_mlp)
    print(f"\nMLP Test Accuracy ({activation}):", mlp_accuracy)
    print("\nClassification Report:\n", classification_report(y_test_encoded, y_pred_mlp, zero_division=1))

from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Label Encoding (0'dan başlayacak şekilde encode ediyorz)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# CNN Modeli için giriş verisinin şekli: (örnek_sayısı, özellik_sayısı, 1)
X_train_cnn = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)
X_test_cnn = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)

activation_functions = ['relu', 'sigmoid', 'tanh']

for activation in activation_functions:
    print(f"\n--- CNN ile {activation.upper()} aktivasyonu ---")
    cnn_model = Sequential()
    cnn_model.add(Input(shape=(X_train_cnn.shape[1], 1)))  # Giriş katmanı
    cnn_model.add(Conv1D(64, kernel_size=3, activation=activation))
    cnn_model.add(MaxPooling1D(pool_size=2))
    cnn_model.add(BatchNormalization())
    cnn_model.add(Dropout(0.3))
    cnn_model.add(Flatten())
    cnn_model.add(Dense(128, activation=activation))
    cnn_model.add(Dropout(0.3))
    cnn_model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))  # Çıkış katmanı

    # Model derleme
    cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Early stopping??
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


    cnn_model.fit(X_train_cnn, y_train_encoded, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])


    y_pred_cnn = np.argmax(cnn_model.predict(X_test_cnn), axis=1)
    cnn_accuracy = accuracy_score(y_test_encoded, y_pred_cnn)
    print(f"\nCNN Test Accuracy ({activation}):", cnn_accuracy)
    print("\nClassification Report:\n", classification_report(y_test_encoded, y_pred_cnn, zero_division=1))

from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization, Input
from keras.callbacks import EarlyStopping

# RNN modeli için giriş verisinin şekli
X_train_rnn = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)
X_test_rnn = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)

activation_functions = ['relu', 'sigmoid', 'tanh']

for activation in activation_functions:
    print(f"\n--- RNN ile {activation.upper()} aktivasyonu ---")
    rnn_model = Sequential()
    rnn_model.add(Input(shape=(X_train_rnn.shape[1], 1)))  # Giriş katmanı
    rnn_model.add(SimpleRNN(64, activation=activation, return_sequences=True))
    rnn_model.add(Dropout(0.3))
    rnn_model.add(SimpleRNN(32, activation=activation))
    rnn_model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))  # Çıkış katmanı


    rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Early Stopping??
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


    rnn_model.fit(X_train_rnn, y_train_encoded, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])


    y_pred_rnn = np.argmax(rnn_model.predict(X_test_rnn), axis=1)
    rnn_accuracy = accuracy_score(y_test_encoded, y_pred_rnn)
    print(f"\nRNN Test Accuracy ({activation}):", rnn_accuracy)
    print("\nClassification Report:\n", classification_report(y_test_encoded, y_pred_rnn, zero_division=1))

from keras.layers import LSTM

# LSTM modeli için giriş verisinin şekli
X_train_lstm = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)
X_test_lstm = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)

for activation in activation_functions:
    print(f"\n--- LSTM ile {activation.upper()} aktivasyonu ---")
    lstm_model = Sequential()
    lstm_model.add(Input(shape=(X_train_lstm.shape[1], 1)))  # Giriş katmanı
    lstm_model.add(LSTM(64, activation=activation, return_sequences=True))
    lstm_model.add(Dropout(0.3))
    lstm_model.add(LSTM(32, activation=activation))
    lstm_model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))  # Çıkış katmanı


    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Early Stopping??
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    lstm_model.fit(X_train_lstm, y_train_encoded, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])


    y_pred_lstm = np.argmax(lstm_model.predict(X_test_lstm), axis=1)
    lstm_accuracy = accuracy_score(y_test_encoded, y_pred_lstm)
    print(f"\nLSTM Test Accuracy ({activation}):", lstm_accuracy)
    print("\nClassification Report:\n", classification_report(y_test_encoded, y_pred_lstm, zero_division=1))

from keras.layers import GRU

# GRU modeli için giriş verisinin şekli
X_train_gru = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)
X_test_gru = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)

for activation in activation_functions:
    print(f"\n--- GRU ile {activation.upper()} aktivasyonu ---")
    gru_model = Sequential()
    gru_model.add(Input(shape=(X_train_gru.shape[1], 1)))  # Giriş katmanı
    gru_model.add(GRU(64, activation=activation, return_sequences=True))
    gru_model.add(Dropout(0.3))
    gru_model.add(GRU(32, activation=activation))
    gru_model.add(Dense(len(np.unique(y_train_encoded)), activation='softmax'))  # Çıkış katmanı


    gru_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Early Stopping ??
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


    gru_model.fit(X_train_gru, y_train_encoded, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])


    y_pred_gru = np.argmax(gru_model.predict(X_test_gru), axis=1)
    gru_accuracy = accuracy_score(y_test_encoded, y_pred_gru)
    print(f"\nGRU Test Accuracy ({activation}):", gru_accuracy)
    print("\nClassification Report:\n", classification_report(y_test_encoded, y_pred_gru, zero_division=1))

import seaborn as sns
import matplotlib.pyplot as plt

# target sınıf dağılımı
plt.figure(figsize=(8, 5))
sns.countplot(x=y)
plt.title('Quality (Target) Dağılımı')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

def plot_comparison(models_dict, title):
    """
    Her bir senaryo için modellerin performans metriklerini bar grafiği ile görselleştirme.
    """
    metrics = ['accuracy', 'f1_score', 'precision', 'recall']
    x = np.arange(len(models_dict))
    width = 0.2

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle(title, fontsize=16)

    for i, metric in enumerate(metrics):
        ax = axes[i // 2, i % 2]
        values = [models_dict[model][metric] for model in models_dict]
        ax.bar(x, values, width=width, color='skyblue', label=metric)
        ax.set_xticks(x)
        ax.set_xticklabels(models_dict.keys(), rotation=45)
        ax.set_title(f'{metric.capitalize()} Comparison')
        ax.set_ylabel(metric.capitalize())
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.show()

# 1. Temel modeller
base_models = {
    'Logistic Regression': {'accuracy': 0.54, 'f1_score': 0.51, 'precision': 0.54, 'recall': 0.54},
    'Decision Tree': {'accuracy': 0.59, 'f1_score': 0.59, 'precision': 0.59, 'recall': 0.59},
    'Random Forest': {'accuracy': 0.69, 'f1_score': 0.68, 'precision': 0.69, 'recall': 0.69},
    'KNN': {'accuracy': 0.56, 'f1_score': 0.55, 'precision': 0.54, 'recall': 0.56},
    'SVM': {'accuracy': 0.57, 'f1_score': 0.54, 'precision': 0.57, 'recall': 0.57},
    'LightGBM': {'accuracy': 0.69, 'f1_score': 0.67, 'precision': 0.68, 'recall': 0.68}
}

# 2. Class weight ve feature selection kullanılan modeller
class_weight_fs_models = {
    'Logistic Regression': {'accuracy': 0.27, 'f1_score': 0.33, 'precision': 0.50, 'recall': 0.27},
    'Decision Tree': {'accuracy': 0.60, 'f1_score': 0.60, 'precision': 0.60, 'recall': 0.60},
    'Random Forest': {'accuracy': 0.70, 'f1_score': 0.68, 'precision': 0.70, 'recall': 0.70},
    'KNN': {'accuracy': 0.56, 'f1_score': 0.55, 'precision': 0.55, 'recall': 0.56},
    'SVM': {'accuracy': 0.29, 'f1_score': 0.34, 'precision': 0.48, 'recall': 0.30},
    'LightGBM': {'accuracy': 0.62, 'f1_score': 0.62, 'precision': 0.63, 'recall': 0.62}
}

# 3. Sınıf birleştirme yapılan modeller
merged_class_models = {
    'Logistic Regression': {'accuracy': 0.54, 'f1_score': 0.52, 'precision': 0.55, 'recall': 0.56},
    'Decision Tree': {'accuracy': 0.60, 'f1_score': 0.60, 'precision': 0.60, 'recall': 0.60},
    'Random Forest': {'accuracy': 0.68, 'f1_score': 0.67, 'precision': 0.69, 'recall': 0.68},
    'KNN': {'accuracy': 0.56, 'f1_score': 0.55, 'precision': 0.55, 'recall': 0.56},
    'SVM': {'accuracy': 0.52, 'f1_score': 0.45, 'precision': 0.42, 'recall': 0.52},
    'LightGBM': {'accuracy': 0.65, 'f1_score': 0.65, 'precision': 0.65, 'recall': 0.65}
}


plot_comparison(base_models, "Temel Modellerin Karşılaştırılması")
plot_comparison(class_weight_fs_models, "Classweight ve Feature Selection Kullanılan Modellerin Karşılaştırılması")
plot_comparison(merged_class_models, "Sınıf Birleştirme Yapılan Modellerin Karşılaştırılması")

import matplotlib.pyplot as plt

# Optimize edilmiş model sonuçları
optimized_models = {
    'KNN': {'accuracy': accuracy_knn_optimized, 'precision': 0.66, 'recall': 0.66, 'f1-score': 0.63},  # Değiştirilen ad
    'Logistic Regression': {'accuracy': accuracy_logistic_optimized, 'precision': 0.52, 'recall': 0.25, 'f1-score': 0.33},
    'SVM': {'accuracy': accuracy_svm_optimized, 'precision': 0.58, 'recall': 0.59, 'f1-score': 0.58},
    'Decision Tree': {'accuracy': accuracy_tree_optimized, 'precision': 0.51, 'recall': 0.44, 'f1-score': 0.45},
    'Random Forest': {'accuracy': accuracy_rf_optimized, 'precision': 0.58, 'recall': 0.52, 'f1-score': 0.54},
    'LightGBM': {'accuracy': accuracy_lgbm_optimized, 'precision': 0.64, 'recall': 0.64, 'f1-score': 0.64}
}

# Performans metriklerini görselleştirme
metrics = ['accuracy', 'precision', 'recall', 'f1-score']

for metric in metrics:
    plt.figure(figsize=(10, 6))
    plt.bar(optimized_models.keys(), [model[metric] for model in optimized_models.values()], color='skyblue')
    plt.title(f"Model Karşılaştırması: {metric.capitalize()}")
    plt.xlabel("Modeller")
    plt.ylabel(f"{metric.capitalize()} Skoru")
    plt.xticks(rotation=15)
    plt.ylim(0, 1)  # 0-1 aralığında olması için
    plt.show()

import matplotlib.pyplot as plt

# Performans metrikleri (örnek değerlere göre doldurulacak)
deep_learning_results = {
    'MLP': {'relu': 0.40, 'sigmoid': 0.40, 'tanh': 0.40},
    'CNN': {'relu': 0.57, 'sigmoid': 0.54, 'tanh': 0.53},
    'RNN': {'relu': 0.57, 'sigmoid': 0.52, 'tanh': 0.53},
    'LSTM': {'relu': 0.54, 'sigmoid': 0.50, 'tanh': 0.53},
    'GRU': {'relu': 0.54, 'sigmoid': 0.54, 'tanh': 0.52}
}

# Grafik çizimi
plt.figure(figsize=(10, 6))
for model, activations in deep_learning_results.items():
    plt.plot(activations.keys(), activations.values(), label=model, marker='o')

plt.title('Derin Öğrenme Algoritmalarının Performans Karşılaştırması')
plt.xlabel('Aktivasyon Fonksiyonu')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Tek tek modeller için karşılaştırma grafiği
for model, activations in deep_learning_results.items():
    plt.figure(figsize=(8, 5))
    plt.bar(activations.keys(), activations.values(), color='skyblue')
    plt.title(f'{model} Modeli Aktivasyon Fonksiyonlarına Göre Performans')
    plt.xlabel('Aktivasyon Fonksiyonu')
    plt.ylabel('Accuracy')
    plt.ylim(0.8, 1.0)  # Ölçeği düzenleyebilirsiniz
    plt.show()